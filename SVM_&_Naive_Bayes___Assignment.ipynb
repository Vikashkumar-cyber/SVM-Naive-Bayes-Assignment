{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Support Vector Machine (SVM), and how does it work?\n",
        "\n",
        "-->  A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. It works by finding an optimal boundary, called a hyperplane, that separates data points into different classes with the largest possible margin.\n",
        "How SVM works\n",
        "The core objective of an SVM is to identify the best possible hyperplane to divide data points from two or more classes. The best hyperplane is the one that achieves the maximum margin, the distance between the hyperplane and the nearest data point from each class.\n",
        "For linearly separable data\n",
        "Optimal hyperplane: For data that can be separated by a straight line or plane, SVM finds the single hyperplane that provides the widest possible separation between the classes.\n",
        "Support vectors: The data points closest to this optimal hyperplane are known as \"support vectors.\" These crucial points determine the position and orientation of the boundary, with all other data points being irrelevant to the final model.\n",
        "Maximum margin: By maximizing the distance between the support vectors and the hyperplane, the SVM creates a robust decision boundary that is less sensitive to small changes in the data.\n",
        "For non-linearly separable data\n",
        "When data cannot be separated by a simple linear boundary, SVM uses a technique called the \"kernel trick\".\n",
        "Kernel trick: This technique involves applying a kernel function to implicitly map the data into a higher-dimensional feature space, where it becomes linearly separable.\n",
        "Higher-dimensional hyperplane: In this new, higher dimension, SVM can find a linear hyperplane to separate the classes. Common kernel functions for this purpose include the Polynomial Kernel and the Radial Basis Function (RBF) Kernel.\n",
        "Handling noise with soft margins\n",
        "For real-world data that contains outliers or noise, a strict boundary can lead to overfitting. SVM can be adapted to be more flexible by using a \"soft margin\".\n",
        "Allowing misclassifications: A soft margin allows for some data points to be misclassified, or to fall on the wrong side of the margin, by introducing a penalty for these errors.\n",
        "The C-parameter: A regularization parameter, \"C,\" controls the trade-off between a wider margin and minimizing misclassification errors. A low C allows more misclassifications for a wider margin, while a high C enforces a stricter boundary.\n",
        "Key takeaways\n",
        "Powerful and versatile: SVMs are effective for both linear and non-linear classification and regression problems, making them applicable in fields like image classification, bioinformatics, and text categorization.\n",
        "Memory efficiency: The decision boundary is defined by a small subset of the training data (the support vectors), making SVM a memory-efficient algorithm.\n",
        "Sensitive to hyperparameters: Performance can be highly dependent on the choice of kernel and regularization parameters, which may require careful tuning.\n",
        "\n",
        "\n",
        "Question 2: Explain the difference between Hard Margin and Soft Margin SVM\n",
        "\n",
        "--> Hard Margin SVM\n",
        "Assumption: Data must be linearly separable, meaning a line (or hyperplane in higher dimensions) can perfectly separate the classes without any errors.\n",
        "Goal: Maximize the margin between the classes while ensuring every single data point is correctly classified and falls outside the margin.\n",
        "Sensitivity: Highly sensitive to outliers, as even a single outlier can significantly impact the decision boundary and lead to overfitting.\n",
        "Use Case: Best for clean, perfectly separable datasets but generally not practical for real-world scenarios.\n",
        "Soft Margin SVM\n",
        "Assumption: The data may not be perfectly linearly separable, or it might contain outliers.\n",
        "Mechanism: Introduces slack variables to allow for some misclassifications and margin violations.\n",
        "Goal: It finds a balance between maximizing the margin and minimizing the number of misclassified points or errors.\n",
        "Flexibility: More flexible and robust to outliers, making it a more practical choice for real-world, noisy datasets.\n",
        "\n",
        "\n",
        "Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and\n",
        "explain its use case.\n",
        "\n",
        "--> The kernel trick is a method used in Support Vector Machines (SVMs) to handle non-linearly separable data by implicitly mapping the data into a higher-dimensional space, where it can be separated by a linear hyperplane, without the computational cost of explicit transformation. A common example is the Radial Basis Function (RBF) kernel, which uses a Gaussian function to measure data point similarity, allowing SVMs to find complex, non-linear decision boundaries suitable for tasks like image classification or detecting localized patterns in data.\n",
        "\n",
        "What is the Kernel Trick\n",
        "\n",
        "Addressing Non-Linearity: SVMs are fundamentally linear classifiers. However, the kernel trick allows them to find non-linear boundaries for data that isn't easily separated by a straight line.\n",
        "Implicit Mapping: Instead of manually mapping every data point to a higher-dimensional space, a kernel function computes the dot product in this transformed space. This transformation is implicit, avoiding the computational expense of working with complex, high-dimensional data.\n",
        "Computational Efficiency: The trick saves significant computational resources because it avoids the direct, often costly, calculations of coordinates in the new, high-dimensional space.\n",
        "Non-Linear Decision Boundaries: By operating in a higher-dimensional space, a linear decision boundary (hyperplane) can effectively separate non-linear data, resulting in a non-linear boundary in the original feature space.\n",
        "\n",
        "\n",
        "Question 4: What is a Naïve Bayes Classifier, and why is it called “naïve”?\n",
        "\n",
        "\n",
        "--> A Naïve Bayes classifier is a probabilistic supervised machine learning algorithm that uses Bayes' Theorem to classify data, but it's called \"naïve\" because it makes the unrealistic assumption that all predictor features are independent of one another, given the class label. Despite this strong and often false assumption, the algorithm performs well in practice, especially for text classification tasks like spam detection, due to its simplicity and efficiency with large datasets.\n",
        "What it is:\n",
        "A Classification Algorithm: It's a type of supervised machine learning model used to assign data points to predefined categories or classes.\n",
        "Based on Bayes' Theorem: The algorithm's core logic is derived from Bayes' Theorem, a fundamental principle in probability that allows for updating the probability of an event based on new information.\n",
        "Probabilistic: It works by calculating the probability of an item belonging to a particular class and then choosing the class with the highest probability.\n",
        "Why it's called \"naïve\":\n",
        "The Assumption of Independence: The \"naïve\" aspect refers to the algorithm's core assumption that each feature used to predict the outcome is conditionally independent of all other features.\n",
        "An Unrealistic Simplification: In real-world scenarios, features are often not truly independent; there are usually dependencies and correlations between them.\n",
        "\"Naive\" Despite Its Success: The term highlights the oversimplified nature of this assumption, which is a significant deviation from reality. However, this simplifying assumption allows the algorithm to be computationally efficient and perform remarkably well in practice, particularly in domains where it's successfully applied.\n",
        "\n",
        "\n",
        "Question 5: Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants.\n",
        "When would you use each one?\n",
        "\n",
        "\n",
        "Gaussian Naïve Bayes is for continuous data that follows a normal distribution, such as height or income. Multinomial Naïve Bayes is for discrete count data, most often used for text classification with term frequencies. Bernoulli Naïve Bayes is for binary (presence/absence) features, suitable for tasks where you only care if a feature is present or not, such as in text classification where you check if a word exists in a document.  \n",
        "Gaussian Naïve Bayes\n",
        "Description: This variant assumes that your continuous features follow a normal (Gaussian) distribution.\n",
        "When to use it: It is ideal for features that are continuous and exhibit a normal distribution, such as a person's height, weight, or income, or even features in text analysis represented by TF-IDF vectors.\n",
        "Multinomial Naïve Bayes\n",
        "Description: This variant is used when features are discrete counts, meaning they represent how many times something occurred.\n",
        "When to use it: It is best for discrete datasets where the features have multiple possible outcomes, such as word frequencies in text classification tasks like spam detection or document classification.\n",
        "Bernoulli Naïve Bayes\n",
        "Description: This variant deals with binary, boolean, or presence/absence features.\n",
        "When to use it: It is best for binary data where features represent the absence or presence of an item, like whether a specific word appears in a document, rather than its frequency.\n",
        "Choosing the Right Variant\n",
        "The most important factor in selecting a Naïve Bayes variant is the nature of your data.\n",
        "If your data is continuous and normally distributed, Gaussian is a good choice.\n",
        "If you have discrete counts, like word counts in text, use Multinomial.\n",
        "If your data features are binary (yes/no, present/absent), use Bernoulli.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "● You can use any suitable datasets like Iris, Breast Cancer, or Wine from\n",
        "sklearn.datasets or a CSV file you have.\n",
        "Question 6: Write a Python program to:\n",
        "● Load the Iris dataset\n",
        "● Train an SVM Classifier with a linear kernel\n",
        "● Print the model's accuracy and support vectors."
      ],
      "metadata": {
        "id": "VDaJNwke81av"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ud5GKbiv8vkH",
        "outputId": "468cf7d8-d05f-41fc-f2d7-608c5e1c6d53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n",
            "Support Vectors:\n",
            "[[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train an SVM Classifier with a linear kernel\n",
        "svm_classifier = SVC(kernel='linear')\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "# Calculate and print the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Print the support vectors\n",
        "print(\"Support Vectors:\")\n",
        "print(svm_classifier.support_vectors_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "● Load the Breast Cancer dataset\n",
        "● Train a Gaussian Naïve Bayes model\n",
        "● Print its classification report including precision, recall, and F1-score"
      ],
      "metadata": {
        "id": "H7YDKXOJFY_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = datasets.load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Gaussian Naïve Bayes model\n",
        "gnb_model = GaussianNB()\n",
        "gnb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = gnb_model.predict(X_test)\n",
        "\n",
        "# Print the classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=breast_cancer.target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6sspvR-FYfu",
        "outputId": "5eccacfe-d961-40c6-bf43-60248d0184ce"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       0.93      0.90      0.92        63\n",
            "      benign       0.95      0.96      0.95       108\n",
            "\n",
            "    accuracy                           0.94       171\n",
            "   macro avg       0.94      0.93      0.94       171\n",
            "weighted avg       0.94      0.94      0.94       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\n",
        "C and gamma.\n",
        "● Print the best hyperparameters and accuracy.\n"
      ],
      "metadata": {
        "id": "YGUiKk8ZFhrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001], 'kernel': ['rbf']}\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=2)\n",
        "\n",
        "# Fit the GridSearchCV object to the training data\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found by GridSearchCV\n",
        "print(\"Best Hyperparameters:\")\n",
        "print(grid.best_params_)\n",
        "\n",
        "# Predict on the test set using the best model\n",
        "y_pred = grid.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the best model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with Best Hyperparameters: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhyHA1w-FkFg",
        "outputId": "9057fb43-2f28-492f-ca2d-6cc9d0d21bfb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
            "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "Best Hyperparameters:\n",
            "{'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "Model Accuracy with Best Hyperparameters: 0.78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "sklearn.datasets.fetch_20newsgroups).\n",
        "● Print the model's ROC-AUC score for its predictions.\n"
      ],
      "metadata": {
        "id": "OedG5QowFndg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "# Load a subset of the 20 Newsgroups dataset\n",
        "# Using a smaller subset for demonstration purposes\n",
        "categories = ['alt.atheism', 'soc.religion.christian']\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n",
        "\n",
        "X_train, y_train = newsgroups_train.data, newsgroups_train.target\n",
        "X_test, y_test = newsgroups_test.data, newsgroups_test.target\n",
        "\n",
        "# Convert text data to TF-IDF features\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Train a Multinomial Naïve Bayes model\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict probabilities on the test set for ROC-AUC score\n",
        "y_pred_proba = nb_model.predict_proba(X_test_tfidf)[:, 1]\n",
        "\n",
        "# Calculate and print the ROC-AUC score\n",
        "# Check if there are at least two classes present in the test set for ROC AUC calculation\n",
        "if len(np.unique(y_test)) > 1:\n",
        "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    print(f\"ROC-AUC Score: {roc_auc:.2f}\")\n",
        "else:\n",
        "    print(\"Cannot calculate ROC-AUC score: Only one class present in the test set.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfL1q1MxFwix",
        "outputId": "47c2eb32-c4d5-4d95-eecf-862766ea556a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you’re working as a data scientist for a company that handles\n",
        "email communications.\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
        "contain:\n",
        "● Text with diverse vocabulary\n",
        "● Potential class imbalance (far more legitimate emails than spam)\n",
        "● Some incomplete or missing data\n",
        "Explain the approach you would take to:\n",
        "● Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "● Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        "● Address class imbalance\n",
        "● Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "PTqkggAOG7RK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# Load a synthetic text dataset with potential imbalance\n",
        "# Using 'comp.graphics' and 'comp.windows.x' to simulate imbalance\n",
        "# (assuming 'comp.graphics' is the minority class for demonstration)\n",
        "categories = ['comp.graphics', 'comp.windows.x']\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n",
        "\n",
        "X_train, y_train = newsgroups_train.data, newsgroups_train.target\n",
        "X_test, y_test = newsgroups_test.data, newsgroups_test.target\n",
        "\n",
        "# Convert text data to TF-IDF features\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000) # Limit features for simplicity\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Check class distribution before handling imbalance\n",
        "print(\"Class distribution before oversampling:\", Counter(y_train))\n",
        "\n",
        "# Address class imbalance using SMOTE (Oversampling)\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_tfidf, y_train)\n",
        "\n",
        "print(\"Class distribution after oversampling:\", Counter(y_train_resampled))\n",
        "\n",
        "# Train a Multinomial Naïve Bayes model on resampled data\n",
        "print(\"\\n--- Multinomial Naïve Bayes ---\")\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_nb = nb_model.predict(X_test_tfidf)\n",
        "y_pred_proba_nb = nb_model.predict_proba(X_test_tfidf)[:, 1]\n",
        "\n",
        "# Evaluate Naïve Bayes model\n",
        "print(\"Classification Report (Naïve Bayes):\")\n",
        "print(classification_report(y_test, y_pred_nb, target_names=newsgroups_test.target_names))\n",
        "\n",
        "# Calculate ROC-AUC for Naïve Bayes\n",
        "if len(np.unique(y_test)) > 1:\n",
        "    roc_auc_nb = roc_auc_score(y_test, y_pred_proba_nb)\n",
        "    print(f\"ROC-AUC Score (Naïve Bayes): {roc_auc_nb:.2f}\")\n",
        "else:\n",
        "    print(\"Cannot calculate ROC-AUC for Naïve Bayes: Only one class present in the test set.\")\n",
        "\n",
        "\n",
        "# Train an SVM Classifier with RBF kernel on resampled data\n",
        "print(\"\\n--- Support Vector Machine (SVM) ---\")\n",
        "svm_model = SVC(kernel='rbf', probability=True, random_state=42) # probability=True to get predict_proba\n",
        "svm_model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_svm = svm_model.predict(X_test_tfidf)\n",
        "y_pred_proba_svm = svm_model.predict_proba(X_test_tfidf)[:, 1]\n",
        "\n",
        "# Evaluate SVM model\n",
        "print(\"Classification Report (SVM):\")\n",
        "print(classification_report(y_test, y_pred_svm, target_names=newsgroups_test.target_names))\n",
        "\n",
        "# Calculate ROC-AUC for SVM\n",
        "if len(np.unique(y_test)) > 1:\n",
        "    roc_auc_svm = roc_auc_score(y_test, y_pred_proba_svm)\n",
        "    print(f\"ROC-AUC Score (SVM): {roc_auc_svm:.2f}\")\n",
        "else:\n",
        "    print(\"Cannot calculate ROC-AUC for SVM: Only one class present in the test set.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsXtx897G89n",
        "outputId": "4b0a87a1-dca5-4469-cf1d-5556d6291ca5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution before oversampling: Counter({np.int64(1): 593, np.int64(0): 584})\n",
            "Class distribution after oversampling: Counter({np.int64(0): 593, np.int64(1): 593})\n",
            "\n",
            "--- Multinomial Naïve Bayes ---\n",
            "Classification Report (Naïve Bayes):\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            " comp.graphics       0.84      0.94      0.89       389\n",
            "comp.windows.x       0.93      0.83      0.88       395\n",
            "\n",
            "      accuracy                           0.88       784\n",
            "     macro avg       0.89      0.88      0.88       784\n",
            "  weighted avg       0.89      0.88      0.88       784\n",
            "\n",
            "ROC-AUC Score (Naïve Bayes): 0.94\n",
            "\n",
            "--- Support Vector Machine (SVM) ---\n",
            "Classification Report (SVM):\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            " comp.graphics       0.83      0.93      0.88       389\n",
            "comp.windows.x       0.92      0.81      0.86       395\n",
            "\n",
            "      accuracy                           0.87       784\n",
            "     macro avg       0.87      0.87      0.87       784\n",
            "  weighted avg       0.87      0.87      0.87       784\n",
            "\n",
            "ROC-AUC Score (SVM): 0.95\n"
          ]
        }
      ]
    }
  ]
}